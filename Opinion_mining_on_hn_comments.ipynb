{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Opinion_mining_on_hn_comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whyeram/Opinion_Mining/blob/main/Opinion_mining_on_hn_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylcp_HS5_fEm"
      },
      "source": [
        "# Website Evaluation using Opinion Mining\n",
        "\n",
        "## Abstract\n",
        "The goal of this project is to evaluate a website using Opinion Mining Techniques. Opinion Mining is the computational study of peopleâ€™s opinions, attitudes and emotions toward an entity. This project develops a system that evaluates a website's sentiment based on the comments by users on a link aggregator website such as [Hacker News](https://news.ycombinator.com). The interaction of users on a link aggregator website gives dense information on the website posted, the comments generated are used in Sentimental Analysis which is equivalent to extracting Opinions from the website. The reason for this indirect approach is due to presence of unstructured media content and scarcity of rich text content in a website, which is challenging for Opinion mining since one has to write systems to interpret all kinds of data type available on a website. It is easier to apply Opinon mining on pure text as comments of users expressing their views on the website posted on link aggregator website.\n",
        "\n",
        "## Project Phases\n",
        "\n",
        "- Data Extraction\n",
        "- Data Cleaning\n",
        "- Sentiment Analysis\n",
        "- Final Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZrRM7i0_fEo"
      },
      "source": [
        "### Data Extraction\n",
        "In order to extract data from a website, first it's content must be downloaded. We use `requests` module which facilitates as a HTTP client library. The downloaded HTML content is then passed into a HTML Parser for efficiently querying the comment text, `BeautifulSoup` library is used for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMEIN2ja_fEp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "5878e061-5e52-42ff-8a35-f157b0313da9"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "hnurl = \"https://hacker-news.firebaseio.com/v0/item/{}.json?print=pretty\"\n",
        "\n",
        "def get_hn_item(itemno):\n",
        "    r = requests.get(hnurl.format(itemno))\n",
        "    if r.status_code == 200:\n",
        "        return r.json()\n",
        "    \n",
        "def get_hn_comments(itemno):\n",
        "    comments = get_hn_items\n",
        "    \n",
        "comments = soup.find_all(\"div\", _class=\"comment\")\n",
        "# comments = list(map(lambda x: x.text.lower(), comments))\n",
        "print(len(comments))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2d641bcfc35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hn_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# comments = list(map(lambda x: x.text.lower(), comments))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7WRwH7s_fEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "94e37161-eab2-4aed-e428-0effac69e13d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=comments, columns=['comments'])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9fec138bb947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comments' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxI_A0lr_fEy"
      },
      "source": [
        "### Data Cleaning\n",
        "Using `nltk` library, the comments are tokenized into words and stopwords are filtered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL56-KSj_fEz",
        "outputId": "a08f5db1-1c10-4d98-fdb1-1604f0f0940a"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# tokenize into words\n",
        "df['tokens'] = df['comments'].map(word_tokenize)\n",
        "\n",
        "# remove stop words and non ascii characters\n",
        "english_stops = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].map(lambda x: [word for word in x if word not in english_stops])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my prediction after the cambridge analytica sc...</td>\n",
              "      <td>[prediction, cambridge, analytica, scandal, br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my key takeaway of cambridge analytica was tha...</td>\n",
              "      <td>[key, takeaway, cambridge, analytica, advertis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&gt;otherwise, the whole point of democracy (one ...</td>\n",
              "      <td>[&gt;, otherwise, ,, whole, point, democracy, (, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&gt; as they've always done through the media? th...</td>\n",
              "      <td>[&gt;, 've, always, done, media, ?, hardest, thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truth: democracy has dependencies. just (fair)...</td>\n",
              "      <td>[truth, :, democracy, dependencies, ., (, fair...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0  my prediction after the cambridge analytica sc...   \n",
              "1  my key takeaway of cambridge analytica was tha...   \n",
              "2  >otherwise, the whole point of democracy (one ...   \n",
              "3  > as they've always done through the media? th...   \n",
              "4  truth: democracy has dependencies. just (fair)...   \n",
              "\n",
              "                                              tokens  \n",
              "0  [prediction, cambridge, analytica, scandal, br...  \n",
              "1  [key, takeaway, cambridge, analytica, advertis...  \n",
              "2  [>, otherwise, ,, whole, point, democracy, (, ...  \n",
              "3  [>, 've, always, done, media, ?, hardest, thin...  \n",
              "4  [truth, :, democracy, dependencies, ., (, fair...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMhezOta_fE3"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2krCN8Q_fE4"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "df['sentiment'] = df['comments'].map(sid.polarity_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Byyvhj_fE7",
        "outputId": "7a8a9f16-d19a-4286-e97d-358b9616d4f3"
      },
      "source": [
        "df['sentiment']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    {'neg': 0.113, 'neu': 0.802, 'pos': 0.085, 'co...\n",
              "1    {'neg': 0.169, 'neu': 0.789, 'pos': 0.042, 'co...\n",
              "2    {'neg': 0.077, 'neu': 0.824, 'pos': 0.1, 'comp...\n",
              "3    {'neg': 0.117, 'neu': 0.847, 'pos': 0.035, 'co...\n",
              "4    {'neg': 0.0, 'neu': 0.758, 'pos': 0.242, 'comp...\n",
              "5    {'neg': 0.137, 'neu': 0.783, 'pos': 0.08, 'com...\n",
              "6    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "7    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "8    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "9    {'neg': 0.024, 'neu': 0.976, 'pos': 0.0, 'comp...\n",
              "Name: sentiment, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKY1ULV_fE-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}